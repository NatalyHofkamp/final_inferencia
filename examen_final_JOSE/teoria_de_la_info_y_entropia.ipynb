{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entropia import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trabajo se adentra en de tres conceptos principales; entropía, información mutua y divergencia de Kullback-Leibler. \n",
    "\n",
    "La entropía, una medida de la incertidumbre y el desorden en un sistema, se utiliza ampliamente en estadísticas, teoría de la información y aprendizaje automático. La información mutua, por otro lado, cuantifica la dependencia entre dos variables aleatorias y encuentra aplicaciones en el análisis de datos y la clasificación. La divergencia de Kullback-Leibler mide la diferencia entre dos distribuciones de probabilidad y es esencial para la comparación y optimización de modelos estadísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_csv(arch):\n",
    "    \"\"\"\n",
    "    Lee un archivo CSV y organiza los datos en dos clases diferentes.\n",
    "\n",
    "    Parameters:\n",
    "        arch (str): Nombre del archivo CSV a leer.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con los datos de la Clase 1 (largo y ancho).\n",
    "        dict: Diccionario con los datos de la Clase 2 (largo y ancho).\n",
    "    \"\"\"\n",
    "    clase1 = {'largo': [], 'ancho': []}\n",
    "    clase2 = {'largo': [], 'ancho': []}\n",
    "\n",
    "    with open(arch, 'r') as a:\n",
    "        lines = a.readlines()\n",
    "        for line in lines[1:]:\n",
    "            line = line.rstrip()\n",
    "            line = line.split(',')\n",
    "            if line[0] == 'C1':\n",
    "                clase1['largo'].append(float(line[1]))\n",
    "                clase1['ancho'].append(float(line[2]))\n",
    "            else:\n",
    "                clase2['largo'].append(float(line[1]))\n",
    "                clase2['ancho'].append(float(line[2]))\n",
    "\n",
    "    return clase1, clase2\n",
    "\n",
    "c1, c2 = leer_csv('dataset_hojas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropía:\n",
    "\n",
    "La entropía se utiliza para cuantificar la incertidumbre asociada a una variable aleatoria discreta. Esta medida proporciona una forma de evaluar cuánta información o sorpresa hay en los posibles resultados de una variable aleatoria. Se representa comúnmente como H(X) para una variable aleatoria X y se calcula utilizando la función de masa de probabilidad p(x), que representa la probabilidad de que X tome un valor específico x. La fórmula general para calcular la entropía de X es:\n",
    "\n",
    "\\begin{equation}\n",
    "H(X) = - Σ p(x) \\log_2(p(x))\n",
    "\\end{equation}\n",
    "\n",
    "Donde:\n",
    "\n",
    "- H(X) es la entropía de la variable aleatoria X.\n",
    "- Σ representa la suma sobre todos los posibles valores x que X puede tomar.\n",
    "- p(x) es la probabilidad de que X tome un valor específico x.\n",
    "- log₂(p(x)) es el logaritmo en base 2 de la probabilidad p(x).\n",
    "\n",
    "La unidad de medida de la entropía en esta fórmula es el bit (unidad de información). En otras palabras, la entropía cuantifica cuántos bits, en promedio, se necesitan para describir el valor de X. Cuanto mayor sea H(X), mayor será la incertidumbre o sorpresa asociada a los valores que puede tomar X. Por otro lado, si H(X) es bajo, significa que los resultados de X son más predecibles y contienen menos incertidumbre.\n",
    "\n",
    "La entropía tiene varias propiedades interesantes, como:\n",
    "\n",
    "- No negativa: H(X) ≥ 0. No puede ser menor que cero y alcanza su mínimo cuando X es completamente determinista (es decir, la probabilidad de un valor es 1 y las demás son 0).\n",
    "- Es máxima cuando todos los valores son igualmente probables: Si todos los valores x tienen la misma probabilidad p(x), entonces H(X) es máximo y alcanza su valor máximo cuando log₂(p(x)) es igual para todos los x.\n",
    "- Es aditiva para variables independientes: Si tienes dos variables aleatorias independientes X e Y, la entropía conjunta H(X, Y) es igual a la suma de sus entropías individuales, es decir, H(X, Y) = H(X) + H(Y), si X e Y con independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_probabilidad(data, bins):\n",
    "    prob_data, _ = np.histogram(data, bins=bins, density=True)\n",
    "    return prob_data\n",
    "\n",
    "def H(p):\n",
    "    entropia = 0\n",
    "    for i in p:\n",
    "        if i > 0:\n",
    "            entropia += -i * np.log2(i)\n",
    "    return entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía largo Clase 1: 0.3907339569703859\n",
      "Entropía ancho Clase 1: 0.6915183236881639\n",
      "Entropía largo Clase 2: 0.4779111613291836\n",
      "Entropía ancho Clase 2: 0.7821578003645895\n"
     ]
    }
   ],
   "source": [
    "prob_l_c1 = calcular_probabilidad(c1['largo'], bins=5)\n",
    "entropia_largo_c1 = H(prob_l_c1)\n",
    "print(\"Entropía largo Clase 1:\", entropia_largo_c1)\n",
    "\n",
    "prob_a_c1 = calcular_probabilidad(c1['ancho'], bins=5)\n",
    "entropia_ancho_c1 = H(prob_a_c1)\n",
    "print(\"Entropía ancho Clase 1:\", entropia_ancho_c1)\n",
    "\n",
    "prob_l_c2 = calcular_probabilidad(c2['largo'], bins=5)\n",
    "entropia_largo_c2 = H(prob_l_c2)\n",
    "print(\"Entropía largo Clase 2:\", entropia_largo_c2)\n",
    "\n",
    "prob_a_c2 = calcular_probabilidad(c2['ancho'], bins=5)\n",
    "entropia_ancho_c2 = H(prob_a_c2)\n",
    "print(\"Entropía ancho Clase 2:\", entropia_ancho_c2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropía Conjunta:\n",
    "\n",
    "La Entropía Conjunta, denotada como H(X, Y), es una medida de la incertidumbre conjunta de dos variables aleatorias discretas, X e Y. Esta métrica cuantifica cuánta información se necesita para describir el comportamiento conjunto de estas variables, considerando la probabilidad conjunta de las diferentes combinaciones de valores que pueden tomar ambas variables.\n",
    "\n",
    "La fórmula para la Entropía Conjunta se expresa como:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "H(X,Y)=−∑∑p(x,y) \\log_2(p(x,y))\n",
    "\\end{aligned} \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "- H(X, Y): Esto representa la entropía conjunta de dos variables aleatorias discretas, X e Y.                                                    \n",
    "- ∑ ∑: Estamos sumando sobre todas las combinaciones posibles de valores de X e Y. La primera suma (Σ) se refiere a la variable X, y la segunda suma (Σ) se refiere a la variable Y.                                                                                                   \n",
    "- p(x, y): Esto representa la probabilidad conjunta de que X tome el valor x e Y tome el valor y. Es la probabilidad de que ocurra una cierta combinación de valores de X e Y.                                                                                                            \n",
    "- log₂(p(x, y)): Estamos calculando el logaritmo en base 2 de la probabilidad conjunta p(x, y).\n",
    "\n",
    "Se basa en la teoría de la información y se relaciona con la medida de la incertidumbre. Cuanto mayor sea el valor de H(X, Y), mayor será la incertidumbre en la relación entre X e Y. Es decir, si H(X, Y) es alto, implica que la probabilidad conjunta p(x, y) está distribuida de manera más uniforme, lo que significa que es difícil predecir el valor de una variable dado el valor de la otra, lo que resulta en una mayor incertidumbre conjunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilidad_conjunta(ancho, largo, bins):\n",
    "    H, x_edges, y_edges = np.histogram2d(ancho, largo, bins)\n",
    "    p = H / np.sum(H)  \n",
    "    return p\n",
    "\n",
    "def h_conj(prob):\n",
    "    h = 0\n",
    "    for row in prob:\n",
    "        for j in row:\n",
    "            if j != 0:  # log(0) es -inf\n",
    "                h -= j * np.log2(j)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropia del largo de c1 y ancho de c1:  3.0321465454645993\n",
      "entropia del largo de c2 y ancho de c2:  2.853608585978931\n",
      "entropia del largo de c1 y ancho de c2:  3.6559265882715515\n",
      "entropia del largo de c2 y ancho de c1:  3.603000890035352\n"
     ]
    }
   ],
   "source": [
    "prob_conjunta_c1 = probabilidad_conjunta(c1['largo'], c1['ancho'], 5)\n",
    "entropia_conj_c1 = h_conj(prob_conjunta_c1)\n",
    "print(\"entropia del largo de c1 y ancho de c1: \", entropia_conj_c1)\n",
    "\n",
    "prob_conjunta_c2 = probabilidad_conjunta(c2['largo'], c2['ancho'], 5)\n",
    "entropia_conj_c2 = h_conj(prob_conjunta_c2)\n",
    "print(\"entropia del largo de c2 y ancho de c2: \", entropia_conj_c2)\n",
    "\n",
    "prob_conjunta_c1_c2 = probabilidad_conjunta(c1['largo'][:len(c2['largo'])], c2['ancho'], 5)\n",
    "entropia_conj_c1_c2 = h_conj(prob_conjunta_c1_c2)\n",
    "print(\"entropia del largo de c1 y ancho de c2: \", entropia_conj_c1_c2)\n",
    "\n",
    "prob_conjunta_c2_c1 = probabilidad_conjunta(c2['largo'], c1['ancho'][:len(c2['ancho'])], 5)\n",
    "entropia_conj_c2_c1 = h_conj(prob_conjunta_c2_c1)\n",
    "print(\"entropia del largo de c2 y ancho de c1: \", entropia_conj_c2_c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropía Condicional:\n",
    "\n",
    "La entropía condicional nos permite cuantificar la incertidumbre en una variable aleatoria dada otra variable aleatoria. Esta medida es crucial para comprender la relación entre variables y cuánta información adicional se requiere cuando conocemos cierta información previa.\n",
    "La fórmula de la entropía condicional H(X∣Y) se define de la siguiente manera:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "H(X∣Y)=−∑_x ∑_y p(x, y) log p(y|x)\n",
    "\\end{aligned} \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "- p(x,y) es la probabilidad conjunta de que X tome el valor x y Y tome el valor y.\n",
    "- p(y∣x) es la probabilidad condicional de que Y tome el valor y dado que X tomó el valor x.\n",
    "\n",
    "Se utiliza para medir cuánta incertidumbre o entropía adicional se presenta en la variable X después de conocer los valores de la variable Y. Si H(X|Y) es alta, significa que X sigue siendo altamente incierto incluso después de observar Y, lo que indica que Y no proporciona mucha información sobre X. Por otro lado, si H(X|Y) es baja, significa que conocer Y reduce significativamente la incertidumbre en X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropia_condicional(ancho,largo, bins):\n",
    "    h = 0\n",
    "    H, x_edges, y_edges = np.histogram2d(ancho,largo, bins, range=((min(ancho), max(ancho)),(min(largo), max(largo))))\n",
    "    p = H / np.sum(H) \n",
    "    p_A = np.sum(H, axis=0)\n",
    "    p_conju= H\n",
    "    p_condi= p_conju/ p_A\n",
    "    h= 0\n",
    "    for x in range(len(p_condi)):\n",
    "        for i in range(len(p_condi)):\n",
    "                if p[x,i] != 0 and p_condi[x,i] !=0:\n",
    "                    h-= p[x,i]*np.log2(p_condi[x,i])\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropia  Condicional (Clase 1): 1.1905916335938092\n",
      "Entropia Condicional (Clase 2): 1.062915871286254\n",
      "Entropía Condicional entre Largo de c1 y Ancho de c2: 1.8652338735788743\n",
      "Entropía Condicional entre Largo de c2 y Ancho de c1: 1.6765397327173592\n"
     ]
    }
   ],
   "source": [
    "entro_prob_condicional_c1 = entropia_condicional(c1['largo'], c1['ancho'], 5)\n",
    "print(\"Entropia  Condicional (Clase 1):\", entro_prob_condicional_c1)\n",
    "\n",
    "entro_prob_condicional_c2 = entropia_condicional(c2['largo'], c2['ancho'], 5)\n",
    "print(\"Entropia Condicional (Clase 2):\", entro_prob_condicional_c2)\n",
    "\n",
    "entro_prob_condicional_largo_c1_ancho_c2 = entropia_condicional(c1['largo'][:len(c2['largo'])], c2['ancho'], 5)\n",
    "print(\"Entropía Condicional entre Largo de c1 y Ancho de c2:\", entro_prob_condicional_largo_c1_ancho_c2)\n",
    "\n",
    "entro_prob_condicional_largo_c2_ancho_c1 = entropia_condicional(c2['largo'], c1['ancho'][:len(c2['ancho'])], 5)\n",
    "print(\"Entropía Condicional entre Largo de c2 y Ancho de c1:\", entro_prob_condicional_largo_c2_ancho_c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divergencia de Kullback-Leibler:\n",
    "\n",
    "La Divergencia de Kullback-Leibler es una medida que se utiliza en teoría de la información y estadísticas para cuantificar la diferencia entre dos distribuciones de probabilidad. En esencia, la KL Divergence se utiliza para medir cuánta información se pierde al aproximar una distribución de probabilidad, \"p\", por otra \"q\".\n",
    "\n",
    "La fórmula general de la KL Divergence entre dos distribuciones de probabilidad discreta p(x) y q(x) se define como:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "D(p∣∣q)=∑_x p(x)log( \\frac{p(x)}{q(x)})\n",
    "\\end{aligned} \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "La interpretación de la KL Divergence es fundamental: cuando su valor es cero, significa que las dos distribuciones son idénticas. Sin embargo, a medida que el valor de la KL Divergence aumenta, refleja una mayor discrepancia entre las distribuciones. En otras palabras, un valor alto indica que la aproximación de una distribución por la otra no es precisa.\n",
    "\n",
    "Es importante destacar que la KL Divergence no es simétrica, a diferencia de métricas como la distancia euclidiana. Esto se traduce en que D(p∣∣q) $\\not \\neq$ D(q∣∣p), lo que demuestra que la KL Divergence se enfoca en la diferencia de información entre las dos distribuciones y que el contexto de comparación es fundamental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropia_relativa(largo, ancho):\n",
    "    return np.mean(np.log2((largo)/ancho))\n",
    "\n",
    "def hacer_arr(data):\n",
    "    d = np.array(data)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergencia de Kullback-Leibler entre c1 largo y c1 ancho:  1.1110046614365088\n",
      "Divergencia de Kullback-Leibler entre c2 largo y c2 ancho:  1.0055477551415855\n",
      "Divergencia de Kullback-Leibler entre c1 largo y c2 ancho:  1.4373347583571323\n",
      "Divergencia de Kullback-Leibler entre c2 largo y c1 ancho:  0.7036005128962612\n"
     ]
    }
   ],
   "source": [
    "print(\"Divergencia de Kullback-Leibler entre c1 largo y c1 ancho: \", entropia_relativa(hacer_arr(c1['largo']), hacer_arr(c1['ancho'])))\n",
    "print(\"Divergencia de Kullback-Leibler entre c2 largo y c2 ancho: \", entropia_relativa(hacer_arr(c2['largo']), hacer_arr(c2['ancho'])))\n",
    "print(\"Divergencia de Kullback-Leibler entre c1 largo y c2 ancho: \", entropia_relativa(hacer_arr(c1['largo'][:len(c2['largo'])]), hacer_arr(c2['ancho'])))\n",
    "print(\"Divergencia de Kullback-Leibler entre c2 largo y c1 ancho: \", entropia_relativa(hacer_arr(c2['largo']), hacer_arr(c1['ancho'][:len(c2['ancho'])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropia_relativa(p, q):\n",
    "    entropy = 0\n",
    "    for x in range(5):\n",
    "        for y in range(5):\n",
    "            if p[x,y] > 0 and q[x,y] >0: \n",
    "                entropy += p[x,y]  * np.log2(p[x,y]  / q[x,y] )        \n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergencia de Kullback-Leibler entre c1 y c2:  0.24180405291993168\n"
     ]
    }
   ],
   "source": [
    "print(\"Divergencia de Kullback-Leibler entre c1 y c2: \", entropia_relativa(prob_conjunta_c1, prob_conjunta_c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Información Mutua: \n",
    "\n",
    "La Información Mutua es una medida que cuantifica la reducción en la incertidumbre de una variable aleatoria X debido al conocimiento o la observación de otra variable aleatoria Y. Esta medida se define como:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "I(X;Y)=H(X)−H(X∣Y)\n",
    "\\end{aligned} \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "- H(X) es la entropía de X, que mide la incertidumbre de X antes de conocer Y.\n",
    "- H(X∣Y) es la entropía condicional de X dado Y, que representa la incertidumbre que queda en X después de conocer Y.\n",
    "\n",
    "La Información Mutua captura cuánta información se gana o se pierde al observar la variable Y en relación con la variable X. Si I(X;Y) es alto, significa que Y proporciona información significativa sobre X. Por otro lado, si I(X;Y) es bajo o cero, implica que Y no aporta mucha información adicional sobre X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info(title, conj, data1, data2, anch1, anch2):\n",
    "    diver = 0\n",
    "    p_x_y = conj\n",
    "    p_x =  data1\n",
    "    p_y = data2\n",
    "    for i in range(anch1):\n",
    "        for j in range(anch2):\n",
    "            if p_x_y[i][j] !=0:\n",
    "                diver += p_x_y[i][j]*(log2(p_x_y[i][j]/(p_x[i]*p_y[j])))\n",
    "\n",
    "    print(title + str(diver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información Mutual de C1 y C2 largo: 0.045912343475816\n",
      "Información Mutual de C1 y C2 ancho: 0.07960086380423345\n",
      "Información Mutual de C1: 0.5300115307683496\n",
      "Información Mutual de C2: 0.6655652637928239\n"
     ]
    }
   ],
   "source": [
    "norm, calc, l_b , a_b = conj(c1['largo'][:len(c2['largo'])], c2['largo'], 5)  \n",
    "marg_1 = calc_marginal(c1['largo'][:len(c2['largo'])], 5)\n",
    "marg_2 = calc_marginal(c2['largo'], 5)\n",
    "mutual_info(\"Información Mutual de C1 y C2 largo: \" , norm, marg_1, marg_2, 5, 5)\n",
    "\n",
    "\n",
    "norm, calc, l_b , a_b = conj(c1['ancho'][:len(c2['ancho'])], c2['ancho'], 5)  \n",
    "marg_1 = calc_marginal(c1['ancho'][:len(c2['ancho'])], 5)\n",
    "marg_2 = calc_marginal(c2['ancho'], 5)\n",
    "mutual_info(\"Información Mutual de C1 y C2 ancho: \" , norm, marg_1, marg_2, 5, 5)\n",
    "\n",
    "\n",
    "norm, calc, l_b , a_b = conj(c1['largo'], c1['ancho'], 5)  \n",
    "marg_1 = calc_marginal(c1['largo'], 5)\n",
    "marg_2 = calc_marginal(c1['ancho'], 5)\n",
    "mutual_info(\"Información Mutual de C1: \" , norm, marg_1, marg_2, 5, 5)\n",
    "\n",
    "\n",
    "norm, calc, l_b , a_b = conj(c2['largo'], c2['ancho'], 5)  \n",
    "marg_1 = calc_marginal(c2['largo'], 5)\n",
    "marg_2 = calc_marginal(c2['ancho'], 5)\n",
    "mutual_info(\"Información Mutual de C2: \" , norm, marg_1, marg_2, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La información mutua es simétrica, lo que significa que también se puede expresar como I(Y;X), reflejando la bidireccionalidad de la relación entre X e Y. Cuando dos variables son independientes, la información mutua es igual a cero, ya que el conocimiento de una variable no aporta información útil sobre la otra. \n",
    "\n",
    "En el contexto de la computación, es importante reconocer que la cantidad de decimales que una computadora puede manejar está limitada, lo que a menudo conlleva a la aproximación o redondeo de números, lo que, a su vez, puede resultar en imprecisiones en los cálculos.\n",
    "\n",
    "En este caso, en la información mutua entre el largo y el ancho en relación con las Clases 1 y 2 se acerca a 0, estoque indica que estas dos características parecen ser independientes. Esto sugiere que no existe una relación sustancial entre el largo y el ancho de los elementos de las clases, y, por lo tanto, estas características no aportan información relevante para distinguir entre las dos clases.\n",
    "\n",
    "En contraste, si la información mutua entre Clase 1 y Clase 2 en función de su largo y ancho se acerca a 1, esto indica que estas características de largo y ancho están altamente correlacionadas con las clases Clase 1 y Clase 2. En otras palabras, las dimensiones del largo y ancho de los elementos proporcionan información valiosa para diferenciar entre las dos clases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pensamiento_computacional",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
