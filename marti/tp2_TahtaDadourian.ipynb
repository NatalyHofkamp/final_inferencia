{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import numpy as np\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>Trabajo Práctico 2: Entropía</h1>\n",
    "    <h2>Inferencia y Estimación</h2>\n",
    "    <p>Alumna: Tahta Dadourian Martina</p>\n",
    "    <p>Correo Electrónico: mtahtadadourian@udesa.edu.ar</p>\n",
    "</div>\n",
    "\n",
    "El siguiente trabajo práctico se centra en el estudio de la entropía de y entre variables, para definir la dependencia o independencia de variables aleatorias. Se utiliza un dataset de mediciones del ancho y largo de hojas de dos clases distintas, y se busca comprender la relación entre estas características dentro de una clase y entre clases. En ambos, se toman 222 muestras.\n",
    "\n",
    "Además, se utiliza el libro *Elements of Information Theory*, escrito por Thomas M. Cover y Joy A. Thomas, como referencia para las definiciones de las ecuaciones y la teoría detrás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_csv(arch):\n",
    "    \"\"\"Lector de archivo con dataset de hojas. \"\"\"\n",
    "    \n",
    "    clase1 = {'largo': [], 'ancho': []}\n",
    "    clase2 = {'largo': [], 'ancho': []}\n",
    "\n",
    "    with open(arch, 'r') as a:\n",
    "        lines = a.readlines()\n",
    "        for line in lines[1:]:\n",
    "            line = line.rstrip()\n",
    "            line = line.split(',')\n",
    "            if line[0] == 'C1':\n",
    "                clase1['largo'].append(float(line[1]))\n",
    "                clase1['ancho'].append(float(line[2]))\n",
    "            else:\n",
    "                clase2['largo'].append(float(line[1]))\n",
    "                clase2['ancho'].append(float(line[2]))\n",
    "\n",
    "    return clase1, clase2\n",
    "\n",
    "c1, c2 = leer_csv('dataset_hojas.csv')\n",
    "size_c2 = len(c2['largo']) #Se achica el tamaño de las muestras de clase 1 para comparar a clase dos, porque tiene más muestras.\n",
    "c1['largo'] = c1['largo'][:size_c2]\n",
    "c1['ancho'] = c1['ancho'][:size_c2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía individual\n",
    "Comenzando con el cálculo de la entropía individual del ancho y largo de cada clase. El libro define a la entropía como una medida de la incerteza de una variable aleatoria. Se calcula la entropía de una variable aleatoria discreta a partir de la ecuación:\n",
    "$$\n",
    "H(X) = - \\sum_{x \\in X} p(x) \\log_2{p(x)},\n",
    "$$\n",
    "siendo $X$ el conjunto de muestras y $p(x)$ la probabilidad de una muestra $x$. En el caso en que $p(x) = 0$, se cumple que $0\\log_2{0} = 0$. Además, por su definición se comprende que $H(x) \\geq 0$.\n",
    "\n",
    "La entropía dice la cantidad de información requerida para describir la variable a la que se la calcula. En el caso de usar el logaritmo en base 2, la entropía dice la cantidad de bits necesarios para describirla.  \n",
    "\n",
    "En el experimento, las entorpía son las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Clase 1 ----\n",
      "H(largo1) = 5.358\n",
      "H(ancho1) = 4.511\n",
      "\n",
      "---- Clase 2 ----\n",
      "H(largo2) = 5.161\n",
      "H(ancho2) = 4.224\n"
     ]
    }
   ],
   "source": [
    "def marg(x, anch):\n",
    "    \"\"\"Cálculo de marginal. \"\"\"\n",
    "\n",
    "    p_x, bin_edges = np.histogram(x, anch)\n",
    "    p_x = p_x/len(x)\n",
    "    return p_x, bin_edges[1]-bin_edges[0]\n",
    "\n",
    "def h(p_x, anch):\n",
    "    \"\"\"Cálculo de entropía de una variable. \"\"\"\n",
    "    h = 0\n",
    "    for p in p_x:\n",
    "        h -= p*log2(p/anch) #/anch para que de la misma entropia para cualquier ancho de bins\n",
    "\n",
    "    return h\n",
    "\n",
    "\n",
    "print('---- Clase 1 ----')\n",
    "marg_l1, anch_l1 = marg(c1['largo'], 5)\n",
    "marg_a1, anch_a1 = marg(c1['ancho'], 5)\n",
    "h_l1 = h(marg_l1, anch_l1)\n",
    "h_a1 = h(marg_a1, anch_a1)\n",
    "print(f'H(largo1) = {round(h_l1, 3)}')\n",
    "print(f'H(ancho1) = {round(h_a1, 3)}')\n",
    "\n",
    "print('\\n---- Clase 2 ----')\n",
    "marg_l2, anch_l2 = marg(c2['largo'], 5)\n",
    "marg_a2, anch_a2 = marg(c2['ancho'], 5)\n",
    "h_l2 = h(marg_l2, anch_l2)\n",
    "h_a2 = h(marg_a2, anch_a2)\n",
    "print(f'H(largo2) = {round(h_l2, 3)}')\n",
    "print(f'H(ancho2) = {round(h_a2, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía conjunta\n",
    "\n",
    "De la misma manera, se puede pensar en un vector aleatorio y medir la entropía conjunta entre sus variables. En este caso, la ecuación es:\n",
    "$$\n",
    "H(X, Y) = - \\sum_{x \\in X}\\sum_{y \\in Y} p(x, y) \\log_2{p(x, y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Clase 1 --------\n",
      "H(largo1, ancho1) = 3.146\n",
      "\n",
      "-------- Clase 2 --------\n",
      "H(largo2, ancho2) = 2.854\n"
     ]
    }
   ],
   "source": [
    "def conjunta(m1, m2, bins):\n",
    "    \"\"\"Calcular la función conjunta (normalizada y original) de dos variables aleatorias con bins. \"\"\"\n",
    "    \n",
    "    conj, _, _ = np.histogram2d(m1, m2, bins)\n",
    "    conj_norm = conj/np.sum(conj)\n",
    "    return conj, conj_norm\n",
    "\n",
    "\n",
    "def h_conj(conj):\n",
    "    \"\"\"Calcular la entropía conjunta. \"\"\"\n",
    "    \n",
    "    h = 0\n",
    "    for f in conj:\n",
    "        for j in f:\n",
    "            if j!=0:\n",
    "                h -= j*(log2(j))\n",
    "    return h\n",
    "\n",
    "conj_c1, conj_c1_norm = conjunta(c1['largo'], c1['ancho'], 5)\n",
    "h_la_1 = h_conj(conj_c1_norm)\n",
    "print('-------- Clase 1 --------')\n",
    "print(f'H(largo1, ancho1) = {round(h_la_1, 3)}')\n",
    "\n",
    "conj_c2, conj_c2_norm = conjunta(c2['largo'], c2['ancho'], 5)\n",
    "h_la_2 = h_conj(conj_c2_norm)\n",
    "print('\\n-------- Clase 2 --------')\n",
    "print(f'H(largo2, ancho2) = {round(h_la_2, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía condicional\n",
    "También, se define la entropía condicional entre dos varibles como:\n",
    "$$\n",
    "H(X|Y) = - \\sum_{x \\in X}\\sum_{y \\in Y} p(x, y) \\log_2{p(y|x)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Clase 1 --------\n",
      "H(ancho1|largo1) = 1.248\n",
      "\n",
      "------- Clase 2 --------\n",
      "H(ancho2|largo2) = 1.125\n",
      "\n",
      "------- Anchos ---------\n",
      "H(ancho1|ancho2) = 1.847\n",
      "\n",
      "------- Largos ---------\n",
      "H(largo1|largo2) = 1.852\n"
     ]
    }
   ],
   "source": [
    "def h_cond(conj):\n",
    "    \"\"\"Cálculo de la entropía condicional. \"\"\"\n",
    "    \n",
    "    p_A = np.sum(conj, axis=0)\n",
    "    p_condi= conj/ p_A\n",
    "    h= 0\n",
    "    for x in range(len(p_condi)):\n",
    "        for i in range(len(p_condi)):\n",
    "                if conj[x][i] != 0 and p_condi[x][i] !=0:\n",
    "                    h-= conj[x][i]*np.log2(p_condi[x][i])\n",
    "    return h\n",
    "\n",
    "print('------- Clase 1 --------')\n",
    "h_cond_a_l_1 = h_cond(np.transpose(conj_c1_norm))\n",
    "print(f'H(ancho1|largo1) = {round(h_cond_a_l_1, 3)}')\n",
    "\n",
    "print('\\n------- Clase 2 --------')\n",
    "h_cond_a_l_2 = h_cond(np.transpose(conj_c2_norm))\n",
    "print(f'H(ancho2|largo2) = {round(h_cond_a_l_2, 3)}')\n",
    "\n",
    "print('\\n------- Anchos ---------')\n",
    "conj_ancho, conj_ancho_norm = conjunta(c1['ancho'], c2['ancho'], 5)\n",
    "h_cond_a = h_cond(conj_ancho_norm)\n",
    "print(f'H(ancho1|ancho2) = {round(h_cond_a, 3)}')\n",
    "\n",
    "print('\\n------- Largos ---------')\n",
    "conj_largo, conj_largo_norm = conjunta(c1['largo'], c2['largo'], 5)\n",
    "h_cond_l = h_cond(conj_largo_norm)\n",
    "print(f'H(largo1|largo2) = {round(h_cond_l, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la entropía condicional y la individual, se puede definir la conjunta de la siguiente manera:\n",
    "$$\n",
    "H(X, Y) = H(X) + H(Y|X)\n",
    "$$\n",
    "\n",
    "Además, si se cumple que son independientes, la condicional $H(Y|X) = H(Y)$, por lo que $H(X, Y) = H(X) + H(Y)$. Por la definición de entropía (la cantidad de bits necesarios para describir una variable), mayor es la diferencia entre la entropía condicional y la individual, mayor es el aporte de la variable que condiciona a la descripición de la variable condicionada.\n",
    "\n",
    "Se prueba esta relación entre las muestras, primero comparando el ancho con el largo de una clase, para ver si son independientes estas características en una hoja. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Clase 1 --------\n",
      "H(ancho1|largo1) = 1.248\n",
      "H(ancho1) = 4.511\n",
      "\n",
      "H(largo1|ancho1) = 1.22\n",
      "H(largo1) = 5.358\n",
      "\n",
      "------- Clase 2 --------\n",
      "H(ancho2|largo2) = 1.125\n",
      "H(ancho2) = 4.224\n",
      "\n",
      "H(largo2|ancho2) = 1.063\n",
      "H(largo2) = 5.161\n"
     ]
    }
   ],
   "source": [
    "print('------- Clase 1 --------')\n",
    "h_cond_l_a_1 = h_cond(conj_c1_norm)\n",
    "print(f'H(ancho1|largo1) = {round(h_cond_a_l_1, 3)}')\n",
    "print(f'H(ancho1) = {round(h_a1, 3)}')\n",
    "\n",
    "print(f'\\nH(largo1|ancho1) = {round(h_cond_l_a_1, 3)}')\n",
    "print(f'H(largo1) = {round(h_l1, 3)}')\n",
    "\n",
    "print('\\n------- Clase 2 --------')\n",
    "h_cond_l_a_2 = h_cond(conj_c2_norm)\n",
    "print(f'H(ancho2|largo2) = {round(h_cond_a_l_2, 3)}')\n",
    "print(f'H(ancho2) = {round(h_a2, 3)}')\n",
    "\n",
    "print(f'\\nH(largo2|ancho2) = {round(h_cond_l_a_2, 3)}')\n",
    "print(f'H(largo2) = {round(h_l2, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ambos casos, la entropía condicional no es muy cercana a la individual, es decir, el largo y ancho de una misma clase se aportan algo de información entre sí. Esto implica que no pueden ser independientes las características. \n",
    "\n",
    "Luego, se prueba con los anchos y largos entre clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- Anchos ---------\n",
      "H(ancho1|ancho2) = 1.847\n",
      "H(ancho1) = 4.511\n",
      "\n",
      "H(ancho2|ancho1) = 1.711\n",
      "H(ancho2) = 4.224\n",
      "\n",
      "------- Largos ---------\n",
      "H(largo1|largo2) = 1.852\n",
      "H(largo1) = 5.358\n",
      "\n",
      "H(largo2|largo1) = 1.683\n",
      "H(largo2) = 5.161\n"
     ]
    }
   ],
   "source": [
    "print('\\n------- Anchos ---------')\n",
    "print(f'H(ancho1|ancho2) = {round(h_cond_a, 3)}')\n",
    "print(f'H(ancho1) = {round(h_a1, 3)}')\n",
    "\n",
    "conj_ancho2, conj_ancho_norm2 = conjunta(c2['ancho'], c1['ancho'], 5)\n",
    "h_cond_a2 = h_cond(conj_ancho_norm2)\n",
    "print(f'\\nH(ancho2|ancho1) = {round(h_cond_a2, 3)}')\n",
    "print(f'H(ancho2) = {round(h_a2, 3)}')\n",
    "\n",
    "print('\\n------- Largos ---------')\n",
    "print(f'H(largo1|largo2) = {round(h_cond_l, 3)}')\n",
    "print(f'H(largo1) = {round(h_l1, 3)}')\n",
    "\n",
    "conj_largo2, conj_largo_norm2 = conjunta(c2['largo'], c1['largo'], 5)\n",
    "h_cond_l2 = h_cond(conj_largo_norm2)\n",
    "print(f'\\nH(largo2|largo1) = {round(h_cond_l2, 3)}')\n",
    "print(f'H(largo2) = {round(h_l2, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, hay diferencia entre las entropías, pero es de menor magnitud que la anterior. Para ambas características es menor a $0.1$ bits. Entre clases, el aporte de las variables que condicionan a la descripición es poco, por lo que las distribuciones de las características son diferentes. Este cálculo sugiere que no se debería rechazar la independencia entre clases.\n",
    "\n",
    "### Entropía relativa\n",
    "La entropía relativa $D(p||q)$, también conocida como Kullback Leibler, es definida en el libro como la divergencia entre dos funciones de probabilidad. Esta medida es siempre positiva y es nula si y solo sí las funciones de probabilidad son iguales. Se calcula de la siguiente manera:\n",
    "$$\n",
    "D(p(x)||q(x)) = \\sum_{x \\in X} p(x) \\log_2{\\frac{p(x)}{q(x)}} = E_p(log_2(\\frac{p(x)}{q(x)})),\n",
    "$$\n",
    "donde se cumple que si $q(x) = 0$, entonces $p(x)\\log_2{\\frac{p(x)}{0} = \\inf}$, y, si $p(x) = 0$, entonces $0\\log_2{\\frac{0}{q(x)} = 0}$.\n",
    "\n",
    "La entropía relativa sirve como medida de la ineficiencia generada por el uso de la función de probabilidad $q(x)$ para la variable aleatoria $x$, cuando su función de distribución, verdaderamente, es $p(x)$. Si se trabaja con bits, la entropía relativa sería la cantidad de bits adicionales necesarios para describir $x$ con $q(x)$, en vez de usar $p(x)$. Sería utilizar $H(x) + D(p||q)$ bits en vez de $H(x)$ bits.\n",
    "\n",
    "De manera parecida, se define la entropía relativa entre dos probabilidades conjuntas, $p(x, y)$ y $q(x, y)$:\n",
    "$$\n",
    "D(p(x, y)||q(x, y)) = \\sum_{x \\in X}\\sum_{y \\in Y} p(x, y) \\log_2{\\frac{p(x, y)}{q(x, y)}},\n",
    "$$\n",
    "\n",
    "Se encuentra la entropía relativa entre clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Clase 1 --------\n",
      "D(p(largo1, ancho1)||q(largo2, ancho2)) = 0.327\n",
      "D(q(largo2, ancho2)||p(largo1, ancho1)) = 0.392\n"
     ]
    }
   ],
   "source": [
    "def h_relat_conj(conj1, conj2, r, c):\n",
    "    \"\"\"Cálculo de entropía relativa de dos probabilidades conjuntas. \"\"\"\n",
    "\n",
    "    d = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            if conj1[i][j]!=0 and conj2[i][j]!=0:\n",
    "                d += conj1[i][j]*(log2(conj1[i][j]/conj2[i][j]))\n",
    "    return d\n",
    "\n",
    "\n",
    "h_relat_clases12 = h_relat_conj(conj_c1_norm, conj_c2_norm, 5, 5)\n",
    "h_relat_clases21 = h_relat_conj(conj_c2_norm, conj_c1_norm, 5, 5)\n",
    "\n",
    "print('------- Clase 1 --------')\n",
    "print(f\"D(p(largo1, ancho1)||q(largo2, ancho2)) = {round(h_relat_clases12, 3)}\")\n",
    "print(f\"D(q(largo2, ancho2)||p(largo1, ancho1)) = {round(h_relat_clases21, 3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por las entropías relativas calculadas, se puede decir que hay una ineficiencia en usar la distribución de la clase 1 para describir a la clase 2, y lo mismo al revés. Lo cual implica que las clases son diferentes. La independencia entre variables de distintas clases es lógica.\n",
    "\n",
    "#### Información mutua\n",
    "A partir de la entropía relativa, se define la información mutua entre dos variables. Esta es una medida de la cantidad de información que una variable contiene de otra. En específico, si se tiene dos variables aleatorias $X$ e $Y$, con función de probabilidad conjunta $p(x, y)$ y marginales $p(x)$ y $p(y)$, se define la información mutua como:\n",
    "$$\n",
    "I(X, Y) = D(p(x, y)||p(x)p(y)) = \\sum_{x \\in X}\\sum_{y \\in Y} p(x, y) \\log_2{\\frac{p(x,y)}{p(x)p(y)}}\n",
    "$$\n",
    "Por la definición de entropía relativa, más chico el número que de la información mutua, más parecidas son las distribuciones $p(x, y)$ y $p(x)p(y)$, es decir, menos información contiene $X$ de $Y$ y al revés. En el caso de que sean iguales, por ende son independientes las variables, la información mutua sería $I(X, Y) = 0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Clase 1 --------\n",
      "I(largo1, ancho1) = 0.678\n",
      "\n",
      "------- Clase 2 --------\n",
      "I(largo2, ancho2) = 0.666\n",
      "\n",
      "------- Anchos ---------\n",
      "I(ancho1, ancho2) = 0.08\n",
      "\n",
      "------- Largos ---------\n",
      "I(largo1, largo2) = 0.046\n"
     ]
    }
   ],
   "source": [
    "def inf_mutua(conj, marg1, marg2, anch1, anch2):\n",
    "    \"\"\"Cálculo de información mutua. \"\"\"\n",
    "    \n",
    "    h = 0\n",
    "    for i in range(anch1):\n",
    "        for j in range(anch2):\n",
    "            if conj[i][j]!=0:\n",
    "                h += conj[i][j]*(log2(conj[i][j]/(marg1[i]*marg2[j])))\n",
    "\n",
    "    return h\n",
    "\n",
    "inf_mutua_c1 = inf_mutua(conj_c1_norm, marg_l1, marg_a1, 5, 5)\n",
    "inf_mutua_c2 = inf_mutua(conj_c2_norm, marg_l2, marg_a2, 5, 5)\n",
    "inf_mutua_ancho = inf_mutua(conj_ancho_norm, marg_a1, marg_a2, 5, 5)\n",
    "inf_mutua_largo = inf_mutua(conj_largo_norm, marg_l1, marg_l2, 5, 5)\n",
    "\n",
    "print('------- Clase 1 --------')\n",
    "print(f\"I(largo1, ancho1) = {round(inf_mutua_c1, 3)}\")\n",
    "\n",
    "print('\\n------- Clase 2 --------')\n",
    "print(f\"I(largo2, ancho2) = {round(inf_mutua_c2, 3)}\")\n",
    "\n",
    "print('\\n------- Anchos ---------')\n",
    "print(f\"I(ancho1, ancho2) = {round(inf_mutua_ancho, 3)}\")\n",
    "\n",
    "print('\\n------- Largos ---------')\n",
    "print(f\"I(largo1, largo2) = {round(inf_mutua_largo, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando los resultados, se puede notar que la información mutua entre el largo y ancho de una clase, que se esperaría tengan dependencia por lo visto anteriormente, es lejano a cero en comparación a la información mutua entre los ancho o largos. La cercanía a cero implica la independencia de las variables, porque el producto de sus marginales es muy parecido a su probabilidad conjunta. Se refuerza que entre caracterísitcas de una misma clase hay dependencia, pero entre clases hay independencia.\n",
    "\n",
    "Otra manera de definir la información mutua es a partir de la marginal y condicional de las variables.\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "I(X, Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n",
    "\\end{aligned} \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "Con los datos, se verifica esta igualdad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Clase 1 --------\n",
      "I(largo1, ancho1) = 0.678\n",
      "H(ancho1) - H(ancho1|largo1) = 3.263\n",
      "\n",
      "------- Clase 2 --------\n",
      "I(largo2, ancho2) = 0.666\n",
      "H(ancho2) - H(ancho2|largo2) = 3.099\n",
      "\n",
      "------- Anchos ---------\n",
      "I(ancho1, ancho2) = 0.08\n",
      "H(ancho1) - H(ancho1|ancho2) = 2.665\n",
      "\n",
      "------- Largos ---------\n",
      "I(largo1, largo2) = 0.046\n",
      "H(largo1) - H(largo1|largo2) = 3.505\n"
     ]
    }
   ],
   "source": [
    "print('------- Clase 1 --------')\n",
    "print(f\"I(largo1, ancho1) = {round(inf_mutua_c1, 3)}\")\n",
    "print(f\"H(ancho1) - H(ancho1|largo1) = {round(h_a1 - h_cond_a_l_1, 3)}\")\n",
    "\n",
    "print('\\n------- Clase 2 --------')\n",
    "print(f\"I(largo2, ancho2) = {round(inf_mutua_c2, 3)}\")\n",
    "print(f\"H(ancho2) - H(ancho2|largo2) = {round(h_a2 - h_cond_a_l_2, 3)}\")\n",
    "\n",
    "print('\\n------- Anchos ---------')\n",
    "print(f\"I(ancho1, ancho2) = {round(inf_mutua_ancho, 3)}\")\n",
    "print(f\"H(ancho1) - H(ancho1|ancho2) = {round(h_a1 - h_cond_a, 3)}\")\n",
    "\n",
    "print('\\n------- Largos ---------')\n",
    "print(f\"I(largo1, largo2) = {round(inf_mutua_largo, 3)}\")\n",
    "print(f\"H(largo1) - H(largo1|largo2) = {round(h_l1 - h_cond_l, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusión\n",
    "En el trabajo se comprende que las variables dentro de una clase son dependientes, pero entre clases son independientes. Esto implica que el largo y ancho de una hoja están relacionados, por lo que conocer una de estas características te dá información sobre la otra. En cambio, entre clases no hay una relación de dependencia. Saber la distribución del largo de una clase no aporta a comprender la distribución de la otra. Ambas conclusiones se comprendieron a partir de resultados que se esperaría obtener de las ditintas entropías en el caso que sean independientes o no. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pensamiento_computacional",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
